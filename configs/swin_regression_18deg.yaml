# Configuration for Swin Transformer Regression on 18-degree dataset
# cd /Users/sbplab/jnrle && python -m angle_estimation_framework.scripts.train --config angle_estimation_framework/configs/swin_regression_18deg.yaml --seed 42

data:
  data_dir: "/Users/sbplab/jnrle/sinetone_link/step_018_sliced"  # 修改為絕對路徑
  # Filtering specific to this dataset/experiment
  material_filter: "plastic"  # or "all", "plastic", "box"
  frequency_filter: [500]
  index_filter: null  # 移除index過濾限制
  format_filter: ["wav"]
  angle_values: [0, 18, 36, 54, 72, 90, 108, 126, 144, 162, 180]
  # Dataset split ratios
  train_split: 0.6
  val_split: 0.2
  # Optional: Define label mapping if needed (e.g., if filenames have string labels)
  # label_mapping: null
  # Optional: Regex to extract label (angle) from filename
  filename_pattern: 'deg([0-9]+)'

audio_params:
  sample_rate: 16000 # Target sample rate (resampling might be needed if source differs)
  n_fft: 1024
  hop_length: 256
  n_mels: 128         # Mel bins (consistent with original Swin script)
  target_length: 128  # Spectrogram time steps (consistent with original Swin script)

model:
  backbone:
    name: "SwinTransformerBackbone"
    params:
      model_name: "microsoft/swin-tiny-patch4-window7-224"
      pretrained: true
      use_feature_mapping: true # Map 1ch spectrogram to 3ch
      freeze_backbone: true     # Start with frozen backbone
      unfreeze_layers: 0        # Number of final stages to unfreeze (0 means none initially)
  head:
    name: "RegressionHead"
    params:
      # input_dim will be inferred from backbone
      hidden_dims: [512, 256]
      dropout_rate: 0.3
      output_dim: 1 # Single value regression output

loss:
  name: "OrderedMarginRankingLoss" # Using the custom combined loss
  use_ranking_loss: false # Set to false as OrderedMarginRankingLoss handles pairs internally
  params:
    # angle_values will be taken from data config
    margin: 1.0
    base_loss_type: 'l1' # Or 'mse'
    base_loss_weight: 1.0
    ranking_loss_weight: 1.0
  # If using StandardMarginRankingLoss or nn.MarginRankingLoss:
  # name: "StandardMarginRankingLoss"
  # use_ranking_loss: true # Set to true to use RankingPairDataset
  # params:
  #   margin: 1.0
  # ranking_params:
  #   num_pairs_train: null # Generate N*(N-1)/2 pairs for train
  #   num_pairs_val: null   # Generate N*(N-1)/2 pairs for val

optimizer:
  name: "AdamW"
  lr: 0.0001
  backbone_lr: 0.00001
  params:
    weight_decay: 0.01
    # Other AdamW params like betas, eps can be added here if needed

scheduler:
  name: "ReduceLROnPlateau" # Example scheduler
  params:
    mode: 'min' # Monitor validation loss
    factor: 0.1
    patience: 5
    min_lr: 1e-7
    verbose: true
  # Example StepLR:
  # name: "StepLR"
  # params:
  #   step_size: 10
  #   gamma: 0.1

training:
  epochs: 3 # 減少更多，以便快速測試
  batch_size: 16 # 減少批次大小
  device: "cpu" # Changed from "cuda" to "cpu"
  num_workers: 0 # Set based on system capability
  # Optional: Fine-tuning schedule
  unfreeze_backbone_epoch: 5 # Epoch to unfreeze backbone layers (requires logic in trainer or script)
  unfreeze_layers_count: 2   # Number of layers to unfreeze at unfreeze_backbone_epoch
  # Optional: Gradient Clipping
  gradient_clipping:
    max_norm: 1.0
  # Optional: Early Stopping
  early_stopping_patience: 1 # 也降低 early stopping 的 patience，以快速測試
  early_stopping_metric: "val_mae" # Monitor validation MAE (lower is better)
  early_stopping_mode: "min"

logging:
  log_dir: "./training_runs" # Directory to save runs
  log_interval: 50 # Print training loss every N batches
  keep_last_checkpoints: 5 # Number of recent checkpoints to keep (null to keep all) 