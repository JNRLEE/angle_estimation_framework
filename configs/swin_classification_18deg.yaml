# Configuration for Swin Transformer Classification on 18-degree dataset
# cd /Users/sbplab/jnrle && python -m angle_estimation_framework.scripts.train --config angle_estimation_framework/configs/swin_classification_18deg.yaml --seed 42


data:
  data_dir: "/Users/sbplab/jnrle/sinetone_link/step_018_sliced"  # 修改為絕對路徑
  # Filtering specific to this dataset/experiment
  material_filter: "plastic"  # or "all", "plastic", "box"
  frequency_filter: [500]
  index_filter: null  # 移除index過濾限制
  format_filter: ["wav"]
  angle_values: [0, 18, 36, 54, 72, 90, 108, 126, 144, 162, 180]
  # Dataset split ratios
  train_split: 0.6
  val_split: 0.2
  # Optional: Define label mapping if needed (e.g., if filenames have string labels)
  # label_mapping: null
  # Optional: Regex to extract label (angle) from filename
  filename_pattern: 'deg([0-9]+)'

audio_params:
  sample_rate: 16000 # Target sample rate (resampling might be needed if source differs)
  n_fft: 1024
  hop_length: 256
  n_mels: 128         # Mel bins (consistent with original Swin script)
  target_length: 128  # Spectrogram time steps (consistent with original Swin script)

model:
  backbone:
    name: "SwinTransformerBackbone"
    params:
      model_name: "microsoft/swin-tiny-patch4-window7-224"
      pretrained: true
      use_feature_mapping: true # Map 1ch spectrogram to 3ch
      freeze_backbone: true     # Start with frozen backbone
      unfreeze_layers: 0        # Number of final stages to unfreeze (0 means none initially)
  head:
    name: "ClassificationHead"  # 改為分類頭
    params:
      # input_dim will be inferred from backbone
      hidden_dims: [512, 256]
      dropout_rate: 0.3
      num_classes: 11  # 對應11個角度類別 [0, 18, 36, 54, 72, 90, 108, 126, 144, 162, 180]

loss:
  name: "CrossEntropyLoss"  # 改為交叉熵損失函數
  use_ranking_loss: false
  params: {}  # CrossEntropyLoss 一般不需要額外參數
  # 如果需要權重（解決類別不平衡）可以使用以下配置
  # params:
  #   weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # 每個類別的權重

optimizer:
  name: "AdamW"
  lr: 0.0001
  backbone_lr: 0.00001
  params:
    weight_decay: 0.01
    # Other AdamW params like betas, eps can be added here if needed

scheduler:
  name: "ReduceLROnPlateau" # Example scheduler
  params:
    mode: "min" # Monitor validation loss
    factor: 0.1
    patience: 5
    min_lr: 1e-7
    verbose: true

training:
  epochs: 3 # 減少更多，以便快速測試
  batch_size: 16 # 減少批次大小
  device: "cpu" # Changed from "cuda" to "cpu"
  num_workers: 0 # Set based on system capability
  # Optional: Fine-tuning schedule
  unfreeze_backbone_epoch: 5 # Epoch to unfreeze backbone layers (requires logic in trainer or script)
  unfreeze_layers_count: 2   # Number of layers to unfreeze at unfreeze_backbone_epoch
  # Optional: Gradient Clipping
  gradient_clipping:
    max_norm: 1.0
  # Optional: Early Stopping
  early_stopping_patience: 1 # 也降低 early stopping 的 patience，以快速測試
  early_stopping_metric: "val_accuracy" # 改為監控準確率而非 MAE
  early_stopping_mode: "max" # 改為最大化準確率（越高越好）

logging:
  log_dir: "./training_runs" # Directory to save runs
  log_interval: 50 # Print training loss every N batches
  keep_last_checkpoints: 5 # Number of recent checkpoints to keep (null to keep all) 